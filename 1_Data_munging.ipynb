{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data munging\n",
    "\n",
    "## Contents\n",
    "1. [Setup](#Section-1%3A-Setup)\n",
    "    1. [Import](#1.1-Import-Packages)\n",
    "    1. [Download](#1.2-Download-Data)\n",
    "    1. [Read](#1.3-Read-the-Data-into-Python-using-Pandas)\n",
    "1. [Cleaning Data](#Section-2%3A-Cleaning-Data)\n",
    "    1. [Selecting Users](#2.1-Selecting-18-59-Year-Old-Users)\n",
    "    1. [Grouping by Age and Height](#2.2-Grouping-People-by-Age-and-Height)\n",
    "    1. [Merging Text](#2.3-Merging-Text)\n",
    "    1. [Grouping Categories](#2.4-Grouping-Categories)\n",
    "    1. [Dropping Columns](#2.5-Dropping-Unneeded-Columns)\n",
    "1. [Saving](#Section-3%3A-Saving-the-Results)\n",
    "\n",
    "## Section 0: Background\n",
    "Most of the time and effort in projects using computational methods goes into data munging, the process of gathering, cleaning, and preparing data for our actual analysis. This notebook walks through that process with the OKCupid public profile data. You may either:\n",
    "1. Step through the code one cell at a time, readign the descriptions to learn how it works, or\n",
    "2. Run all the code here to prepare data for the other labs / notebooks. \n",
    "    - **Note:** You should check for errors after you run this. Common errors and what to do about them are documented below. \n",
    "\n",
    "### 0.1 Data\n",
    "- We'll be using some profile data that OKCupid has made publically available.\n",
    "- The data is anonymized.\n",
    "- The data includes:\n",
    "    - Demographic information like age, gender, race/ethnicity, height, and income\n",
    "    - Profile text (what people wrote about themselves)    \n",
    "    \n",
    "@Author: [Jeff Lockhart](http://www-personal.umich.edu/~jwlock/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup\n",
    "### 1.1 Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install the gdown package using pip\n",
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import os.path\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Download data\n",
    "- This code checks whether you have downloaded the data yet.\n",
    "- If you have not, it downloads the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You already downloaded the data. Great!\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(\"data/profiles.csv.zip\"):\n",
    "    print(\"You already downloaded the data. Great!\")\n",
    "else:\n",
    "    print('Downloading file...')\n",
    "    gdown.download(id=\"1A9S9PrYe3jNKqqK2yyrUNA3aHzfAhQZS\",\n",
    "                               output=\"data/profiles.csv.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Read the Data into Python using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadZipFile",
     "evalue": "File is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-be82a8fbbe11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprofiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/profiles.csv.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1267\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RealGetContents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1270\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m                 \u001b[0;31m# set the modified flag so central directory gets written\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/zipfile.py\u001b[0m in \u001b[0;36m_RealGetContents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1334\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mendrec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBadZipFile\u001b[0m: File is not a zip file"
     ]
    }
   ],
   "source": [
    "profiles = pd.read_csv('data/profiles.csv.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Cleaning Data\n",
    "\n",
    "In a broad sense, data cleaning refers to any actions that transform the original data we have into the version that that we need for a specific analysis we have in mind. The key point is that what amounts to data cleaning on the specific analysis. For example, in many linguistic analyes that use natural language processing techniques, we tend to remove stop words (For example a, the, and, etc.). But some analyses, such as those study linguistic style, rely on these words. In these cases, we don't want to remove stop words.\n",
    "\n",
    "### 2.1 Selecting 18-59 Year Old Users\n",
    "- Here we select within the profiles using the square brakcets `[]`.\n",
    "- We select cases where two things are true: `age < 60` and `age > 17`. The round brackets `()` are used to group our conditions, so that python knows `&` doesn't just mean the two things closest to it, i.e. `60 & profiles.age` (that code would not make sense).\n",
    "- We save the result, our list of profiles that meet those conditions, as our new `profiles` variable with the `=`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We started with\", profiles.shape[0], \"profiles.\")\n",
    "\n",
    "profiles = profiles[(profiles.age < 60) & (profiles.age > 17)]\n",
    "\n",
    "print(\"After selecting just those between 18 and 59, we have\", \n",
    "      profiles.shape[0], \"left.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Grouping People by Age and Height\n",
    "\n",
    "#### Defining functions to recode data\n",
    "- To change someone's exact age to their age group, we want to do something to the number. We can define the thing we want to do as a `function`. \n",
    "- Here are two functions that take some age `a` and convert it to an age group using these steps:\n",
    "    1. Divide someone's age by 10 (e.g. 34 becomes 3.4)\n",
    "    2. Convert that age to an integer (e.g. 3.4 becomes 3)\n",
    "    3. Multiply that integer by 10 (e.g. 3 becomes 30)\n",
    "    4. Turn that integer into a string / text \n",
    "- `group_age()` and `group_age_long()` do exactly the same thing, but the long one writes out each step so it is easier to follow.\n",
    "- The last line gives the function the number 34. As we wanted, it prints the result `'30'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_age_long(a):\n",
    "    g = a / 10    #divide age by 10\n",
    "    g = int(g)    #convert age to an integer \n",
    "    g = g * 10    #multiply age by 10\n",
    "    g = str(g)    #convert the number to text / string\n",
    "    return g      #return the result\n",
    "\n",
    "def group_age(a):\n",
    "    return str(int(a/10)*10)\n",
    "\n",
    "print(group_age_long(34))\n",
    "print(group_age(34))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply\n",
    "- Apply is a powerful tool built into pandas. The logic is simple. Each person has an age, and we'd like to `apply` our age-grouping function to each one of those ages. \n",
    "- This code says \"go to the profiles data, look at the age column in it, and apply `group_age` to each of the things there.\n",
    "- We create a new column in the profiles data called `age_group`, and we set it equal to the result of the `group_age` function.\n",
    "- We then print the head of the relevant columns in order to check that we did it correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "profiles['age_group'] = profiles.age.apply(group_age)\n",
    "profiles[['age', 'age_group']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Height\n",
    "1. We define a function, `height()`, that returns `under_6` if someone is under 6 feet (72 inches) tall and `over_6` otherwise.\n",
    "2. We apply that function to the column of our data that contains users' height.\n",
    "3. We save the result in a new column called `height_group`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def height(inches):\n",
    "    h = 'under_6'\n",
    "    if inches >= 72:\n",
    "        h = 'over_6'\n",
    "    return h\n",
    "\n",
    "profiles['height_group'] = profiles.height.apply(height)\n",
    "profiles[['height', 'height_group']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Merging Text\n",
    "- The OKC data has 10 different columns with profile text, one for each long-answer question in users' profiles. We want to look at all of the profile text, so this merges it all together in a new column called `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which columns have text in them\n",
    "essay_cols = ['essay0', 'essay1', 'essay2', 'essay3', 'essay4', 'essay5', 'essay6', \n",
    "              'essay7', 'essay8', 'essay9']\n",
    "\n",
    "def concat(row, cols):\n",
    "    tmp = []\n",
    "    for c in cols:\n",
    "        tmp.append(str(row[c]))\n",
    "    new = '\\n'.join(tmp)\n",
    "    return new\n",
    "\n",
    "profiles['text'] = profiles.apply(concat, axis=1, cols=essay_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Grouping Categories\n",
    "- Much of our data comes in many or complicated categories. For example, there are more than 20 different levels of education in the education column. \n",
    "- We want to group these together so that they are easier to interpret\n",
    "\n",
    "#### This cell defines what categories we want and which people belong in them.\n",
    "- Each column we will recode has been given its own dictionary.\n",
    "- The keys of that dictionary are the new categories we want to make.\n",
    "- The value of each key is a list of values that should all be combined in that category.\n",
    "- In many cases, I have left out a default category to simplify the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# education\n",
    "ed_levels = {'<HS': ['dropped out of high school', 'working on high school'],\n",
    "             'HS': ['graduated from high school', 'working on college/university', \n",
    "                    'two-year college', 'dropped out of college/university', \n",
    "                    'high school'], \n",
    "             'BA': ['graduated from college/university', \n",
    "                    'working on masters program', 'working on ph.d program', \n",
    "                    'college/university', 'working on law school', \n",
    "                    'dropped out of masters program', \n",
    "                    'dropped out of ph.d program', 'dropped out of law school', \n",
    "                    'dropped out of med school'],\n",
    "             'Grad_Pro': ['graduated from masters program',\n",
    "                          'graduated from ph.d program',                           \n",
    "                          'graduated from law school', \n",
    "                          'graduated from med school', 'masters program', \n",
    "                          'ph.d program', 'law school', 'med school']\n",
    "            }\n",
    "\n",
    "#body type\n",
    "bodies = {'average': ['average'], \n",
    "          'fit': ['fit', 'athletic', 'jacked'], \n",
    "          'thin': ['thin', 'skinny'], \n",
    "          'overweight': ['curvey', 'a little extra', 'full figured', 'overweight']\n",
    "         }\n",
    "\n",
    "# smoking\n",
    "smoke = {'no': ['no'], np.nan: ['nan']}\n",
    "\n",
    "# Has children\n",
    "kids = {'yes': ['has a kid', 'has kids']}\n",
    "\n",
    "#has pets\n",
    "has_pets = {'yes': ['has']}\n",
    "\n",
    "# alcohol use\n",
    "drinks = {'no': ['rarely', 'not at all']}\n",
    "\n",
    "# drug use\n",
    "drugs = {'no': ['never']}\n",
    "\n",
    "# Employment sector\n",
    "jobs = {'education': ['student', 'education'], \n",
    "        'STEM': ['science', 'computer'], \n",
    "        'business': ['sales', 'executive', 'banking'], \n",
    "        'creative': ['artistic', 'entertainment'], \n",
    "        'med_law': ['medicine', 'law'],\n",
    "        np.nan: ['nan']\n",
    "       }\n",
    "\n",
    "# religion \n",
    "religion = {'none': ['agnosticism', 'atheism'],\n",
    "            'catholicism': ['catholicism'],\n",
    "            'christianity': ['christianity'],\n",
    "            'judaism': ['judaism'],\n",
    "            'buddhism': ['buddhism'],\n",
    "            np.nan: ['nan']\n",
    "           }\n",
    "\n",
    "# languages spoken. People who list more than one langage have a comma between them\n",
    "# thus anyone with a comma in the languages column has more than one language\n",
    "languages = {'multiple': [',']}\n",
    "\n",
    "# race/ethnicity for exact matching\n",
    "ethn = {'White': ['white', 'middle eastern', 'middle eastern, white'], \n",
    "        'Asian': ['asian', 'indian', 'asian, pacific islander'], \n",
    "        'Black': ['black']\n",
    "       }   \n",
    "\n",
    "# race/ethnicityfor fuzzy matching\n",
    "ethn2 = {'Latinx': ['latin'], 'multiple': [','], np.nan: ['nan']}   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for figuring out which group someone belongs in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recode(text, dictionary, default=np.nan):\n",
    "    '''Function for recoding categories in a column based on exact matches'''\n",
    "    out = default\n",
    "    text = str(text)\n",
    "    \n",
    "    for x in dictionary.keys():\n",
    "        for y in dictionary[x]:\n",
    "            if y == text: #exact match\n",
    "                out = x\n",
    "                return out\n",
    "    return out\n",
    "\n",
    "def recode_fuzzy(text, dictionary, default=np.nan):\n",
    "    '''Function for recoding categories in a column based on partial matches'''\n",
    "    out = default\n",
    "    text = str(text)\n",
    "    \n",
    "    for x in dictionary.keys():\n",
    "        for y in dictionary[x]:\n",
    "            if y in text: #partial match\n",
    "                out = x\n",
    "                return out\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the functions and the categories we defined to recode the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles['edu'] = profiles.education.apply(recode, dictionary=ed_levels, \n",
    "                                            default='unknown')\n",
    "profiles['kids'] = profiles.offspring.apply(recode_fuzzy, dictionary=kids, \n",
    "                                            default='no')\n",
    "profiles['smoker'] = profiles.smokes.apply(recode, dictionary=smoke, \n",
    "                                            default='yes')\n",
    "profiles['body'] = profiles.body_type.apply(recode, dictionary=bodies, \n",
    "                                            default='unknown')\n",
    "profiles['alcohol_use'] = profiles.drinks.apply(recode, dictionary=drinks, \n",
    "                                            default='yes')\n",
    "profiles['drug_use'] = profiles.drugs.apply(recode, dictionary=drugs, \n",
    "                                            default='yes')\n",
    "profiles['industry'] = profiles.job.apply(recode_fuzzy, dictionary=jobs, \n",
    "                                            default='other')\n",
    "profiles['religion'] = profiles.religion.apply(recode_fuzzy, dictionary=religion, \n",
    "                                            default='other')\n",
    "profiles['languages'] = profiles.speaks.apply(recode_fuzzy, dictionary=languages, \n",
    "                                            default='English_only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "profiles[['education', 'edu']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More complex recoding: race and ethnicity\n",
    "- Recoding race/ethnicity is more complex than most other variables. Indeed, it is a hotly debated topic in social science. \n",
    "- Here, we use a combination of exact matching and fuzzy matching to approximate the 2010 US Census categories.\n",
    "    - This function deviates from the census by creating exclusive Latinx category. \n",
    "    - Selecting just 'latin' and nothing else was the 3rd most frequent ethnicity in this data. \n",
    "    - The discision to include people who identified as both 'latin' and another race in 'Latinix' is based in research on Latinx people's experience with the US Census, but like all racial and ethnic categorization systems, it is flawed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def census_2010_ethnicity(t):\n",
    "    text = str(t)\n",
    "    e = recode(text, ethn, default='other')\n",
    "    if 'other' == e:\n",
    "        e = recode_fuzzy(text, ethn2, default='other')\n",
    "    return e\n",
    "\n",
    "profiles['race_ethnicity'] = profiles.ethnicity.apply(census_2010_ethnicity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another odd case: pets\n",
    "- The pets data is a bit different than all the other data, so it was easier to write a separate function for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_pets(t, criterion='has'):\n",
    "    '''Function for determining which pets someone has or likes'''\n",
    "    d = False\n",
    "    c = False\n",
    "    t = str(t)\n",
    "    p = 'neither'\n",
    "    if t == 'nan':\n",
    "        p = np.nan\n",
    "    \n",
    "    if 'has dogs' in t:\n",
    "        d = True\n",
    "    if 'has cats' in t:\n",
    "        c = True\n",
    "        \n",
    "    if criterion == 'likes':\n",
    "        if 'likes dogs' in t:\n",
    "            if 'dislikes dogs' not in t:\n",
    "                d = True\n",
    "        if 'likes cats' in t:\n",
    "            if 'dislikes cats' not in t:\n",
    "                c = True\n",
    "        \n",
    "    if c and d:\n",
    "        p = 'both'\n",
    "    elif c:\n",
    "        p = 'cats'\n",
    "    elif d:\n",
    "        p = 'dogs'\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles['pets_likes'] = profiles.pets.apply(which_pets, criterion='likes')\n",
    "profiles['pets_has'] = profiles.pets.apply(which_pets, criterion='has')\n",
    "profiles['pets_any'] = profiles.pets.apply(recode_fuzzy, dictionary=has_pets, \n",
    "                                            default='no')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Dropping Unneeded Columns\n",
    "- We created a lot of new columns so far, and we're not interested in all of the old columns. \n",
    "- This code selects just the columns we want to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Initial columns:\\n', profiles.columns.values)\n",
    "profiles = profiles[['age_group', 'age', 'body', 'alcohol_use', 'drug_use', 'edu', \n",
    "                     'race_ethnicity', 'height_group', 'industry', 'kids', \n",
    "                     'orientation', 'pets_likes', 'pets_has', 'pets_any', \n",
    "                     'religion', 'sex', 'smoker', 'languages', 'text', 'essay0', \n",
    "                     'essay1', 'essay2', 'essay3', 'essay4', 'essay5', 'essay6', \n",
    "                     'essay7', 'essay8', 'essay9']]\n",
    "print('Final columns:\\n', profiles.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "## Section 3: Saving the Results\n",
    "This cell saves the cleaned up data to a file so we can use it again later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles.to_csv('data/clean_profiles.tsv', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
